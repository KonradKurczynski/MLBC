---
title: "Exammple 1"
author: "Konrad Kurczynski"
date: "`r Sys.Date()`"
output: 
  md_document:
    variant: markdown_github
---

```{r}
library(MLBC)
```

# Example 1

## Parameters 

```{r}
nsim  <- 1000
n     <- 16000
m     <- 1000
p     <- 0.05
kappa <- 1
fpr    <- kappa / sqrt(n)

b0    <- 10
b1    <- 1
a0    <- 0.3
a1    <- 0.5

alpha <- c(0.0, 0.5, 0.5)
beta  <- c(0.0, 2.0, 4.0)

# pre-allocate storage
B <- array(0, dim = c(nsim, 9, 2))
S <- array(0, dim = c(nsim, 9, 2))

update_results <- function(coefs, vcovmat, i, method_idx) {
  se <- sqrt(pmax(diag(vcovmat), 0))
  B[i, method_idx, ] <<- coefs
  S[i, method_idx, ] <<- se
}
```

## Data Generation Process

```{r}
generate_data <- function(n, m, p, fpr, b0, b1, a0, a1) {
  N    <- n + m
  X    <- numeric(N)
  Xhat <- numeric(N)
  u    <- runif(N)

  for (j in seq_len(N)) {
    if      (u[j] <= fpr)           X[j]   <- 1
    else if (u[j] <= 2 * fpr)       Xhat[j]<- 1
    else if (u[j] <= p + fpr) {     # true positive
      X[j]   <- 1
      Xhat[j]<- 1
    }
    # otherwise X[j] and Xhat[j] stay 0
  }

  eps <- rnorm(N)   # N(0,1)
  # heteroskedastic noise: σ₁ when X=1, σ₀ when X=0
  Y <- b0 + b1 * X + (a1 * X + a0 * (1 - X)) * eps

  # return *un*-intercepted matrices
  train_Y   <- Y[       1:n    ]
  train_X   <- matrix(Xhat[    1:n], ncol = 1, dimnames = list(NULL, "Xhat"))
  test_Y    <- Y[(n + 1):N    ]
  test_Xhat <- matrix(Xhat[(n+1):N], ncol = 1, dimnames = list(NULL, "Xhat"))
  test_X    <- matrix(X[(n+1):N],    ncol = 1, dimnames = list(NULL, "X"))

  list(
    train_Y   = train_Y,
    train_X   = train_X,
    test_Y    = test_Y,
    test_Xhat = test_Xhat,
    test_X    = test_X
  )
}
```
## Estimation and bias correction

```{r}
for (i in seq_len(nsim)) {
  
  dat <- generate_data(n, m, p, fpr, b0, b1, a0, a1)
  
  train_df <- data.frame(
    Y    = dat$train_Y,
    Xhat = dat$train_X[,1]
  )
  
  test_df <- data.frame(
    Y    = dat$test_Y,
    Xhat = dat$test_Xhat[,1],
    X    = dat$test_X[,1]
  )
  
  # 1) OLS on unlabeled (X̂)
  fit1 <- ols(Y ~ Xhat, data = train_df)
  update_results(fit1$coef, fit1$vcov, i, 1)
  
  # 2) OLS on labeled (true X)
  fit2 <- ols(Y ~ X, data = test_df)
  update_results(fit2$coef, fit2$vcov, i, 2)
  
  # 3–8) Additive & multiplicative bias‑corrections
  fpr_hat <- mean((test_df$Xhat == 1) & (test_df$X == 0))
  for (j in seq_along(alpha)) {
    fpr_bayes <- (fpr_hat * m + alpha[j]) /
      (m + alpha[j] + beta[j])
    
    fit_bca <- ols_bca(Y ~ Xhat,
                       data      = train_df,
                       fpr       = fpr_bayes,
                       m         = m)
    update_results(fit_bca$coef, fit_bca$vcov, i, 2 + j)
    
    fit_bcm <- ols_bcm(Y ~ Xhat,
                       data      = train_df,
                       fpr       = fpr_bayes,
                       m         = m)
    update_results(fit_bcm$coef, fit_bcm$vcov, i, 5 + j)
  }
  
  # 9) One‑step estimator
  fit_os <- one_step(Y ~ Xhat,
                     data          = train_df,
                     homoskedastic = FALSE,
                     distribution  = "normal",
                     intercept     = TRUE)
  update_results(fit_os$coef, fit_os$cov, i, 9)
  
  if (i %% 100 == 0) {
    message("Completed ", i, " of ", nsim, " sims")
  }
}
```

```{r}
coverage <- function(bgrid, b, se) {
  n_grid <- length(bgrid)
  cvg    <- numeric(n_grid)
  for (i in seq_along(bgrid)) {
    val      <- bgrid[i]
    cvg[i]   <- mean(abs(b - val) <= 1.96 * se)
  }
  cvg
}

true_beta1 <- 1.0

methods <- c(
  "OLS θ̂"  = 1,
  "OLS θ"  = 2,
  "BCA-0"  = 3,
  "BCA-1"  = 4,
  "BCA-2"  = 5,
  "BCM-0"  = 6,
  "BCM-1"  = 7,
  "BCM-2"  = 8,
  "OSU"    = 9
)

cov_dict <- sapply(methods, function(col) {
  slopes <- B[, col, 1]   
  ses    <- S[, col, 1]
  mean(abs(slopes - true_beta1) <= 1.96 * ses)
})

cov_series <- setNames(cov_dict, names(methods))
print(cov_series)

```

```{r}
method_names <- names(methods)

coef_names <- c("Beta1","Beta0")

nmethods <- dim(B)[2]
df <- data.frame(Method = method_names, stringsAsFactors = FALSE)

df$Est_Beta1   <- NA_real_
df$SE_Beta1    <- NA_real_
df$CI95_Beta1  <- NA_character_
df$Est_Beta0   <- NA_real_
df$SE_Beta0    <- NA_real_
df$CI95_Beta0  <- NA_character_

for(i in seq_len(nmethods)) {
  est1 <- B[, i, 1]; se1 <- S[, i, 1]
  est0 <- B[, i, 2]; se0 <- S[, i, 2]
  
  ci1 <- quantile(est1, probs = c(0.025, 0.975))
  ci0 <- quantile(est0, probs = c(0.025, 0.975))
  
  df$Est_Beta1[i]  <- mean(est1)
  df$SE_Beta1[i]   <- mean(se1)
  df$CI95_Beta1[i] <- sprintf("[%0.3f, %0.3f]", ci1[1], ci1[2])
  
  df$Est_Beta0[i]  <- mean(est0)
  df$SE_Beta0[i]   <- mean(se0)
  df$CI95_Beta0[i] <- sprintf("[%0.3f, %0.3f]", ci0[1], ci0[2])
}

print(df)
```


```{r}
n     <- 200
p     <- 0.3
Xtrue <- rbinom(n, 1, p)
# ML regressor with 10% false positives
Xhat  <- ifelse(runif(n) < 0.10, 1 - Xtrue, Xtrue)
Y     <- 1 + 2 * Xtrue + rnorm(n)

# 2) Small validation set to estimate fpr
m        <- 50
Xval_t   <- rbinom(m, 1, p)
Xval_h   <- ifelse(runif(m) < 0.10, 1 - Xval_t, Xval_t)
fpr_hat  <- mean(Xval_h == 1 & Xval_t == 0)

# 3) One‐step TMB estimator (Normal), with intercept
fit <- one_step(
  Y            = Y,
  Xhat         = matrix(Xhat, ncol = 1, dimnames = list(NULL, "Xhat")),
  homoskedastic = FALSE,
  distribution  = "normal",
  intercept     = TRUE
)
print(fit)
```

